{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2356e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANOVA (Analysis of Variance) relies on several key assumptions to ensure the validity of the results. If these assumptions are violated, the test may lead to inaccurate conclusions. Below are the assumptions and examples of potential violations:\\n\\n### 1. **Independence of Observations**\\n   - **Assumption**: Each observation must be independent, meaning the value of one observation does not influence or relate to another.\\n   - **Example of Violation**: If the data points are from repeated measures on the same subject or from groups where individuals influence each other (e.g., family members or classmates), the independence assumption is violated.\\n\\n### 2. **Homogeneity of Variances (Equal Variances)**\\n   - **Assumption**: The variances of the groups being compared should be approximately equal.\\n   - **Example of Violation**: If one group has much more variability than another, the assumption of homogeneity of variances is violated. This can lead to misleading F-statistics and inflated Type I error rates. For example, comparing the exam scores of students from different schools where one school has a much wider range of student abilities could violate this assumption.\\n\\n### 3. **Normality of Residuals**\\n   - **Assumption**: The residuals (differences between observed and predicted values) should be normally distributed.\\n   - **Example of Violation**: If the residuals show skewness or heavy tails (i.e., are not normally distributed), this assumption is violated. This can happen, for example, if the data contain extreme outliers or are heavily skewed. Non-normality can impact the validity of the F-test, especially with small sample sizes.\\n\\n### 4. **Random Sampling**\\n   - **Assumption**: The data should be collected through random sampling to ensure generalizability.\\n   - **Example of Violation**: If a biased sampling method is used (e.g., selecting only students from a single, high-performing school when studying general education levels), the results may not be valid for the entire population.\\n\\n### 5. **Additivity and Linearity**\\n   - **Assumption**: The relationship between the dependent variable and independent factors should be linear and additive.\\n   - **Example of Violation**: If interactions between variables are present or if the relationship is not linear (e.g., quadratic or exponential), the assumption is violated. This could lead to an inaccurate interpretation of group differences.\\n\\n### Impact of Violations:\\n- **Independence**: Violations lead to biased results, as the observations are not truly independent.\\n- **Homogeneity of Variance**: If the variances are not equal, it could lead to inflated Type I errors (false positives), especially with unbalanced groups (i.e., groups of unequal sizes).\\n- **Normality of Residuals**: Violating normality can make the F-test less reliable, particularly for small samples, as ANOVA relies on normal distribution theory.\\n- **Random Sampling**: Lack of random sampling affects the generalizability of the results.\\n- **Additivity and Linearity**: Misinterpreting the relationship between variables can lead to incorrect conclusions about the significance of group differences.\\n\\n### Remedies for Violations:\\n- **Transformations**: Applying data transformations (e.g., logarithmic or square root) can help address violations of normality and homogeneity of variance.\\n- **Non-parametric alternatives**: If assumptions are severely violated, consider using non-parametric tests like the Kruskal-Wallis test, which does not assume normality or equal variances.\\n- **Use of Robust Methods**: Some statistical software allows for \"robust\" ANOVA, which adjusts for heteroscedasticity (unequal variances).'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1...\n",
    "\"\"\"ANOVA (Analysis of Variance) relies on several key assumptions to ensure the validity of the results. If these assumptions are violated, the test may lead to inaccurate conclusions. Below are the assumptions and examples of potential violations:\n",
    "\n",
    "### 1. **Independence of Observations**\n",
    "   - **Assumption**: Each observation must be independent, meaning the value of one observation does not influence or relate to another.\n",
    "   - **Example of Violation**: If the data points are from repeated measures on the same subject or from groups where individuals influence each other (e.g., family members or classmates), the independence assumption is violated.\n",
    "\n",
    "### 2. **Homogeneity of Variances (Equal Variances)**\n",
    "   - **Assumption**: The variances of the groups being compared should be approximately equal.\n",
    "   - **Example of Violation**: If one group has much more variability than another, the assumption of homogeneity of variances is violated. This can lead to misleading F-statistics and inflated Type I error rates. For example, comparing the exam scores of students from different schools where one school has a much wider range of student abilities could violate this assumption.\n",
    "\n",
    "### 3. **Normality of Residuals**\n",
    "   - **Assumption**: The residuals (differences between observed and predicted values) should be normally distributed.\n",
    "   - **Example of Violation**: If the residuals show skewness or heavy tails (i.e., are not normally distributed), this assumption is violated. This can happen, for example, if the data contain extreme outliers or are heavily skewed. Non-normality can impact the validity of the F-test, especially with small sample sizes.\n",
    "\n",
    "### 4. **Random Sampling**\n",
    "   - **Assumption**: The data should be collected through random sampling to ensure generalizability.\n",
    "   - **Example of Violation**: If a biased sampling method is used (e.g., selecting only students from a single, high-performing school when studying general education levels), the results may not be valid for the entire population.\n",
    "\n",
    "### 5. **Additivity and Linearity**\n",
    "   - **Assumption**: The relationship between the dependent variable and independent factors should be linear and additive.\n",
    "   - **Example of Violation**: If interactions between variables are present or if the relationship is not linear (e.g., quadratic or exponential), the assumption is violated. This could lead to an inaccurate interpretation of group differences.\n",
    "\n",
    "### Impact of Violations:\n",
    "- **Independence**: Violations lead to biased results, as the observations are not truly independent.\n",
    "- **Homogeneity of Variance**: If the variances are not equal, it could lead to inflated Type I errors (false positives), especially with unbalanced groups (i.e., groups of unequal sizes).\n",
    "- **Normality of Residuals**: Violating normality can make the F-test less reliable, particularly for small samples, as ANOVA relies on normal distribution theory.\n",
    "- **Random Sampling**: Lack of random sampling affects the generalizability of the results.\n",
    "- **Additivity and Linearity**: Misinterpreting the relationship between variables can lead to incorrect conclusions about the significance of group differences.\n",
    "\n",
    "### Remedies for Violations:\n",
    "- **Transformations**: Applying data transformations (e.g., logarithmic or square root) can help address violations of normality and homogeneity of variance.\n",
    "- **Non-parametric alternatives**: If assumptions are severely violated, consider using non-parametric tests like the Kruskal-Wallis test, which does not assume normality or equal variances.\n",
    "- **Use of Robust Methods**: Some statistical software allows for \"robust\" ANOVA, which adjusts for heteroscedasticity (unequal variances).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e508c88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are three main types of ANOVA (Analysis of Variance), and each is used in different situations depending on the number of factors and the relationships between them. Here's an overview of the three types and their typical use cases:\\n    1. One way anova\\n    2. two way anova\\n    3. repeted measures anova\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2...\n",
    "\"\"\"There are three main types of ANOVA (Analysis of Variance), and each is used in different situations depending on the number of factors and the relationships between them. Here's an overview of the three types and their typical use cases:\n",
    "    1. One way anova\n",
    "    2. two way anova\n",
    "    3. repeted measures anova\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44e4db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.655905931471352, 0.02821532555294891)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample weight loss data (replace this with actual data)\n",
    "# Data for each diet group\n",
    "np.random.seed(0)  # for reproducibility\n",
    "diet_A = np.random.normal(loc=5.0, scale=1.5, size=50)  # diet A\n",
    "diet_B = np.random.normal(loc=6.0, scale=1.5, size=50)  # diet B\n",
    "diet_C = np.random.normal(loc=5.5, scale=1.5, size=50)  # diet C\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Diet_A': diet_A,\n",
    "    'Diet_B': diet_B,\n",
    "    'Diet_C': diet_C\n",
    "})\n",
    "\n",
    "# Perform the one-way ANOVA test\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Output the F-statistic and p-value\n",
    "f_statistic, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af4d152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C(Software)</th>\n",
       "      <td>8.947992</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.152926</td>\n",
       "      <td>0.859023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(Experience)</th>\n",
       "      <td>164.553588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.624627</td>\n",
       "      <td>0.026076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(Software):C(Experience)</th>\n",
       "      <td>102.162565</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.746016</td>\n",
       "      <td>0.195908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>702.141835</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               sum_sq    df         F    PR(>F)\n",
       "C(Software)                  8.947992   2.0  0.152926  0.859023\n",
       "C(Experience)              164.553588   1.0  5.624627  0.026076\n",
       "C(Software):C(Experience)  102.162565   2.0  1.746016  0.195908\n",
       "Residual                   702.141835  24.0       NaN       NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data generation\n",
    "np.random.seed(0)\n",
    "# 30 employees with equal representation in each group\n",
    "n_employees = 30\n",
    "\n",
    "# Factors\n",
    "software = ['A']*10 + ['B']*10 + ['C']*10\n",
    "experience = ['novice']*5 + ['experienced']*5  # 5 novice, 5 experienced per program\n",
    "\n",
    "# Simulated time to complete the task (replace with actual data)\n",
    "time_A_novice = np.random.normal(loc=20, scale=5, size=5)\n",
    "time_A_experienced = np.random.normal(loc=18, scale=5, size=5)\n",
    "time_B_novice = np.random.normal(loc=22, scale=5, size=5)\n",
    "time_B_experienced = np.random.normal(loc=19, scale=5, size=5)\n",
    "time_C_novice = np.random.normal(loc=21, scale=5, size=5)\n",
    "time_C_experienced = np.random.normal(loc=20, scale=5, size=5)\n",
    "\n",
    "# Combine the data\n",
    "times = np.concatenate([time_A_novice, time_A_experienced, time_B_novice, \n",
    "                        time_B_experienced, time_C_novice, time_C_experienced])\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Software': software,\n",
    "    'Experience': experience*3,  # Repeating for each software\n",
    "    'Time': times\n",
    "})\n",
    "\n",
    "# Two-way ANOVA\n",
    "model = ols('Time ~ C(Software) * C(Experience)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Output the ANOVA table\n",
    "anova_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee6cc79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST (Total Sum of Squares): 44.0\n",
      "SSE (Explained Sum of Squares): 38.0\n",
      "SSR (Residual Sum of Squares): 6.0\n"
     ]
    }
   ],
   "source": [
    "#11..\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data: groups and values\n",
    "data = {'Group': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n",
    "        'Value': [5, 7, 6, 8, 10, 9, 10, 12, 11]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Overall mean\n",
    "overall_mean = df['Value'].mean()\n",
    "\n",
    "# Group means\n",
    "group_means = df.groupby('Group')['Value'].mean()\n",
    "\n",
    "# SST (Total Sum of Squares)\n",
    "sst = np.sum((df['Value'] - overall_mean) ** 2)\n",
    "\n",
    "# SSE (Explained Sum of Squares)\n",
    "sse = np.sum(df.groupby('Group').apply(lambda x: len(x) * (x['Value'].mean() - overall_mean) ** 2))\n",
    "\n",
    "# SSR (Residual Sum of Squares)\n",
    "ssr = np.sum(df.groupby('Group').apply(lambda x: np.sum((x['Value'] - x['Value'].mean()) ** 2)))\n",
    "\n",
    "# Print results\n",
    "print(f\"SST (Total Sum of Squares): {sst}\")\n",
    "print(f\"SSE (Explained Sum of Squares): {sse}\")\n",
    "print(f\"SSR (Residual Sum of Squares): {ssr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69d5fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -1.6677351961320235\n",
      "p-value: 0.09856078338184605\n",
      "Fail to reject the null hypothesis. There is no significant difference between the two groups.\n"
     ]
    }
   ],
   "source": [
    "#12...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulating data: test scores for control and experimental groups\n",
    "np.random.seed(0)  # For reproducibility\n",
    "control_group = np.random.normal(loc=70, scale=10, size=50)  # Traditional method\n",
    "experimental_group = np.random.normal(loc=75, scale=10, size=50)  # New method\n",
    "\n",
    "# Two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Print t-test results\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the t-test result\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference between the two groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference between the two groups.\")\n",
    "\n",
    "# If significant, follow up with post-hoc test (Tukey's HSD)\n",
    "if p_value < alpha:\n",
    "    # Combine the data into a single array and create a group label array\n",
    "    data = np.concatenate([control_group, experimental_group])\n",
    "    groups = ['Control'] * len(control_group) + ['Experimental'] * len(experimental_group)\n",
    "\n",
    "    # Create a dataframe for the post-hoc test\n",
    "    df = pd.DataFrame({'Score': data, 'Group': groups})\n",
    "\n",
    "    # Tukey's HSD post-hoc test\n",
    "    tukey = pairwise_tukeyhsd(endog=df['Score'], groups=df['Group'], alpha=alpha)\n",
    "\n",
    "    # Print the results of the post-hoc test\n",
    "    print(tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3095b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Anova\n",
      "===================================\n",
      "      F Value Num DF  Den DF Pr > F\n",
      "-----------------------------------\n",
      "Store  0.9395 2.0000 58.0000 0.3967\n",
      "===================================\n",
      "\n",
      "No significant difference in sales between the stores.\n"
     ]
    }
   ],
   "source": [
    "#13..\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulate daily sales data for three stores across 30 days\n",
    "np.random.seed(0)\n",
    "store_a_sales = np.random.normal(loc=200, scale=10, size=30)\n",
    "store_b_sales = np.random.normal(loc=210, scale=10, size=30)\n",
    "store_c_sales = np.random.normal(loc=205, scale=10, size=30)\n",
    "\n",
    "# Create a dataframe for the repeated measures ANOVA\n",
    "data = pd.DataFrame({\n",
    "    'Day': np.tile(np.arange(1, 31), 3),\n",
    "    'Store': np.repeat(['A', 'B', 'C'], 30),\n",
    "    'Sales': np.concatenate([store_a_sales, store_b_sales, store_c_sales])\n",
    "})\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "# Each day is a \"subject\", and each store is a repeated measure\n",
    "rm_anova = AnovaRM(data, 'Sales', 'Day', within=['Store'])\n",
    "anova_results = rm_anova.fit()\n",
    "\n",
    "# Print the ANOVA results\n",
    "print(anova_results)\n",
    "\n",
    "# If the ANOVA is significant, follow up with a post-hoc test\n",
    "alpha = 0.05\n",
    "if anova_results.anova_table['Pr > F'][0] < alpha:\n",
    "    print(\"The repeated measures ANOVA is significant. Performing post-hoc test...\")\n",
    "    \n",
    "    # Post-hoc test using Tukey's HSD\n",
    "    tukey = pairwise_tukeyhsd(endog=data['Sales'], groups=data['Store'], alpha=alpha)\n",
    "    print(tukey)\n",
    "else:\n",
    "    print(\"No significant difference in sales between the stores.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ed9cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn a **two-way ANOVA**, we investigate how two independent variables (factors) affect a dependent variable and whether there is an interaction effect between the two factors. The main effects test the individual influence of each factor, while the interaction effect examines if the factors jointly affect the dependent variable.\\n\\n### Steps:\\n1. **Main Effects**: These test the independent effects of each factor on the dependent variable.\\n2. **Interaction Effects**: This tests whether the combined levels of the two factors produce an effect that is different from the sum of their individual effects.\\n\\nYou can perform a two-way ANOVA using Python\\'s `statsmodels` library, which allows for modeling both main effects and interaction effects.\\n\\n### Example Code:\\n\\nLet\\'s assume we are testing how two factors—**Store** and **Promotion**—affect **daily sales**. The two factors are:\\n- **Factor 1 (Store)**: Store A, Store B, Store C.\\n- **Factor 2 (Promotion)**: Promotion 1, Promotion 2.\\n\\n### Step-by-Step Implementation\\n\\n```python\\nimport pandas as pd\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\n\\n# Simulated data\\nnp.random.seed(0)\\ndata = pd.DataFrame({\\n    \\'Store\\': np.repeat([\\'A\\', \\'B\\', \\'C\\'], 30),\\n    \\'Promotion\\': np.tile(np.repeat([\\'P1\\', \\'P2\\'], 15), 3),\\n    \\'Sales\\': np.random.normal(loc=200, scale=10, size=90) + np.repeat([10, 20, 15], 30)  # Adding variation by store\\n})\\n\\n# Create the model\\n# Formula: Sales ~ Store + Promotion + Store:Promotion\\nmodel = ols(\\'Sales ~ Store + Promotion + Store:Promotion\\', data=data).fit()\\n\\n# Perform ANOVA\\nanova_table = sm.stats.anova_lm(model, typ=2)\\n\\n# Print the ANOVA table\\nprint(anova_table)\\n```\\n\\n### Explanation:\\n1. **Data**:\\n   - `Store`: Three stores (A, B, C).\\n   - `Promotion`: Two promotion types (P1, P2).\\n   - `Sales`: Randomly generated sales data, with different means per store to simulate variation.\\n\\n2. **Model Setup**:\\n   - We use the formula `Sales ~ Store + Promotion + Store:Promotion`:\\n     - `Store`: Main effect for Store.\\n     - `Promotion`: Main effect for Promotion.\\n     - `Store:Promotion`: Interaction effect between Store and Promotion.\\n\\n3. **ANOVA Table**:\\n   - The function `sm.stats.anova_lm(model, typ=2)` produces the ANOVA table.\\n   - `typ=2` indicates that we are using a Type II ANOVA, which is appropriate when the model does not include interaction terms or if interaction terms are independent of each other.\\n\\n### Interpreting the Output:\\nThe ANOVA table will provide:\\n- **Sum of Squares (SS)**: Measures the variance explained by each factor.\\n- **Degrees of Freedom (df)**: The number of independent values.\\n- **F-statistic**: Tests the significance of the effect.\\n- **p-value**: Tells us whether the effect is significant (usually if `p < 0.05`).\\n\\n### Example Output (Hypothetical):\\n```bash\\n                      sum_sq    df         F    PR(>F)\\nStore                1425.25   2.0   15.203     0.0001\\nPromotion            1265.75   1.0   27.456     0.0000\\nStore:Promotion       330.10   2.0    3.582     0.0315\\nResidual            3689.88   84.0       NaN        NaN\\n```\\n\\n- **Store**: Significant main effect (`p = 0.0001`), indicating that the sales differ between the stores.\\n- **Promotion**: Significant main effect (`p = 0.0000`), suggesting that different promotions have a significant effect on sales.\\n- **Store:Promotion**: Significant interaction effect (`p = 0.0315`), meaning that the effect of promotion depends on the store.\\n\\n### Post-hoc Analysis (If Interaction is Significant):\\nIf the interaction effect is significant, you may want to investigate which specific pairs of factor levels differ significantly. You can use **Tukey’s HSD** for pairwise comparisons.\\n\\n```python\\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\\n\\n# Perform Tukey\\'s HSD for post-hoc analysis\\ntukey = pairwise_tukeyhsd(endog=data[\\'Sales\\'], groups=data[\\'Store\\'] + \" & \" + data[\\'Promotion\\'], alpha=0.05)\\nprint(tukey)\\n```\\n\\n### Summary:\\n- The **main effects** (Store and Promotion) test whether each factor individually affects the dependent variable.\\n- The **interaction effect** tests whether the combined levels of Store and Promotion jointly affect sales.\\n- If significant, you can follow up with post-hoc tests (e.g., Tukey’s HSD) to determine where the differences lie.\\n\\nLet me know if you\\'d like to explore further or run this code with specific data!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5...\n",
    "\"\"\"\n",
    "In a **two-way ANOVA**, we investigate how two independent variables (factors) affect a dependent variable and whether there is an interaction effect between the two factors. The main effects test the individual influence of each factor, while the interaction effect examines if the factors jointly affect the dependent variable.\n",
    "\n",
    "### Steps:\n",
    "1. **Main Effects**: These test the independent effects of each factor on the dependent variable.\n",
    "2. **Interaction Effects**: This tests whether the combined levels of the two factors produce an effect that is different from the sum of their individual effects.\n",
    "\n",
    "You can perform a two-way ANOVA using Python's `statsmodels` library, which allows for modeling both main effects and interaction effects.\n",
    "\n",
    "### Example Code:\n",
    "\n",
    "Let's assume we are testing how two factors—**Store** and **Promotion**—affect **daily sales**. The two factors are:\n",
    "- **Factor 1 (Store)**: Store A, Store B, Store C.\n",
    "- **Factor 2 (Promotion)**: Promotion 1, Promotion 2.\n",
    "\n",
    "### Step-by-Step Implementation\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(0)\n",
    "data = pd.DataFrame({\n",
    "    'Store': np.repeat(['A', 'B', 'C'], 30),\n",
    "    'Promotion': np.tile(np.repeat(['P1', 'P2'], 15), 3),\n",
    "    'Sales': np.random.normal(loc=200, scale=10, size=90) + np.repeat([10, 20, 15], 30)  # Adding variation by store\n",
    "})\n",
    "\n",
    "# Create the model\n",
    "# Formula: Sales ~ Store + Promotion + Store:Promotion\n",
    "model = ols('Sales ~ Store + Promotion + Store:Promotion', data=data).fit()\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **Data**:\n",
    "   - `Store`: Three stores (A, B, C).\n",
    "   - `Promotion`: Two promotion types (P1, P2).\n",
    "   - `Sales`: Randomly generated sales data, with different means per store to simulate variation.\n",
    "\n",
    "2. **Model Setup**:\n",
    "   - We use the formula `Sales ~ Store + Promotion + Store:Promotion`:\n",
    "     - `Store`: Main effect for Store.\n",
    "     - `Promotion`: Main effect for Promotion.\n",
    "     - `Store:Promotion`: Interaction effect between Store and Promotion.\n",
    "\n",
    "3. **ANOVA Table**:\n",
    "   - The function `sm.stats.anova_lm(model, typ=2)` produces the ANOVA table.\n",
    "   - `typ=2` indicates that we are using a Type II ANOVA, which is appropriate when the model does not include interaction terms or if interaction terms are independent of each other.\n",
    "\n",
    "### Interpreting the Output:\n",
    "The ANOVA table will provide:\n",
    "- **Sum of Squares (SS)**: Measures the variance explained by each factor.\n",
    "- **Degrees of Freedom (df)**: The number of independent values.\n",
    "- **F-statistic**: Tests the significance of the effect.\n",
    "- **p-value**: Tells us whether the effect is significant (usually if `p < 0.05`).\n",
    "\n",
    "### Example Output (Hypothetical):\n",
    "```bash\n",
    "                      sum_sq    df         F    PR(>F)\n",
    "Store                1425.25   2.0   15.203     0.0001\n",
    "Promotion            1265.75   1.0   27.456     0.0000\n",
    "Store:Promotion       330.10   2.0    3.582     0.0315\n",
    "Residual            3689.88   84.0       NaN        NaN\n",
    "```\n",
    "\n",
    "- **Store**: Significant main effect (`p = 0.0001`), indicating that the sales differ between the stores.\n",
    "- **Promotion**: Significant main effect (`p = 0.0000`), suggesting that different promotions have a significant effect on sales.\n",
    "- **Store:Promotion**: Significant interaction effect (`p = 0.0315`), meaning that the effect of promotion depends on the store.\n",
    "\n",
    "### Post-hoc Analysis (If Interaction is Significant):\n",
    "If the interaction effect is significant, you may want to investigate which specific pairs of factor levels differ significantly. You can use **Tukey’s HSD** for pairwise comparisons.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's HSD for post-hoc analysis\n",
    "tukey = pairwise_tukeyhsd(endog=data['Sales'], groups=data['Store'] + \" & \" + data['Promotion'], alpha=0.05)\n",
    "print(tukey)\n",
    "```\n",
    "\n",
    "### Summary:\n",
    "- The **main effects** (Store and Promotion) test whether each factor individually affects the dependent variable.\n",
    "- The **interaction effect** tests whether the combined levels of Store and Promotion jointly affect sales.\n",
    "- If significant, you can follow up with post-hoc tests (e.g., Tukey’s HSD) to determine where the differences lie.\n",
    "\n",
    "Let me know if you'd like to explore further or run this code with specific data!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341aab70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4f914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
